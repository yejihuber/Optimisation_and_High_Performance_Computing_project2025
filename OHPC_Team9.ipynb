{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b916b777",
   "metadata": {},
   "source": [
    "# **Project: Solar Cycle**\n",
    "\n",
    "**Course:** Optimisation and High performance Computing (OHPC-HS25-AD23)  \n",
    "**Team Members:** Cieplinski Nicole, Plos Penelope, Yeji Huber\n",
    "**Date:** 16.01.2026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3902cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pathlib import Path\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976f916",
   "metadata": {},
   "source": [
    "## Simulated Annealing (SA) function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b98b7ec",
   "metadata": {},
   "source": [
    "### SA function for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01a400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sa_tune(x0, T0, sigma, f, n_iter = 2e5, thinning = 1):\n",
    "\n",
    "    x = x0.copy()           # initial x\n",
    "    T = T0                  # initial temperature\n",
    "    n_params = x0.shape[0]  # number of parameters to optimized\n",
    "\n",
    "    # Means and covariance matrix for the jump distribution -> multivariate normal with mean 0 and standard deviation sigma \n",
    "    means = np.full(n_params, 0)\n",
    "    cov_matrix = np.diag(np.full(n_params, sigma))\n",
    "\n",
    "    # Calculate size of the output array after thinning\n",
    "    # (thinning -> save states at regular intervals instead of every iteration) \n",
    "    # Thinning is by defaut 1, and size_out = n_iter\n",
    "    size_out = int((n_iter + thinning -1)//thinning)    # equivalent to ceiling (n_iter/thinning)\n",
    "    v = np.zeros((size_out, n_params))\n",
    "    # Store the initial parameter array\n",
    "    v[0,:] = x\n",
    "\n",
    "    iter_counter = 0\n",
    "    iter_counter_thin = 0\n",
    "    print(\"Initial loss:\", f(x))\n",
    "    #start main loop\n",
    "    while iter_counter < n_iter:\n",
    "        iter_counter += 1;\n",
    "        x_old = x;\n",
    "        x_proposal = x_old + np.random.multivariate_normal(means, cov_matrix)\n",
    "        DeltaE = f(x_proposal) - f(x_old)\n",
    "        #Metropolis accept/reject step \n",
    "        if np.exp(-np.clip(DeltaE, -100, 100)) >= np.random.rand():\n",
    "            x = x_proposal\n",
    "        else:\n",
    "            x = x_old\n",
    "        \n",
    "        # Update temperature according to schedule\n",
    "        T = T0 * (1 - iter_counter/n_iter)\n",
    "        # Keep track of accepted state\n",
    "        if iter_counter%1 == 0:\n",
    "            print(\"Iteration\", iter_counter, \" - Temperature:\", T, \"Loss\", f(x))\n",
    "        if iter_counter%thinning == 0:\n",
    "            v[iter_counter_thin,:] = x\n",
    "            iter_counter_thin += 1\n",
    "    \n",
    "    return v\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7cab24",
   "metadata": {},
   "source": [
    "### SA function for final optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sa_optimize(x0, T0, sigma, f, n_iter = 2.5e5, burn_in = 2e5):\n",
    "\n",
    "    x = x0.copy()           # initial x\n",
    "    T = T0                  # initial temperature\n",
    "    n_params = x0.shape[0]  # number of parameters to optimized\n",
    "    \n",
    "    # means and covariance matrix for the jump distribution -> multivariate normal\n",
    "    means = np.full(n_params, 0)\n",
    "    cov_matrix = np.diag(np.full(n_params, sigma))\n",
    "\n",
    "    # Size of the output array after burn_in\n",
    "    size_out = int(n_iter - burn_in)\n",
    "    v = np.zeros((size_out, n_params))\n",
    "    \n",
    "    iter_counter = 0\n",
    "    print(\"Initial loss:\", f(x))\n",
    "    # Start main loop\n",
    "    while iter_counter < n_iter:\n",
    "        iter_counter += 1;\n",
    "        x_old = x;\n",
    "        x_proposal = x_old + np.random.multivariate_normal(means, cov_matrix)\n",
    "        DeltaE = f(x_proposal) - f(x_old)\n",
    "        # Metropolis accept/reject step\n",
    "        if np.exp(-np.clip(DeltaE/T, -100,100)) >= np.random.rand():\n",
    "            x = x_proposal\n",
    "        else:\n",
    "            x = x_old\n",
    "        # Update temperature according to schedule\n",
    "        T = T0*(1-iter_counter/n_iter)\n",
    "        # keep track of the algorithm state\n",
    "        if iter_counter%10 == 0:\n",
    "            print(\"Iteration \", iter_counter, \" - Temperature:\", T, \" - Loss:\", f(x))\n",
    "        if iter_counter > burn_in:\n",
    "            v[iter_counter-int(burn_in)-1, :] = x\n",
    "\n",
    "    return v "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4da089",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ccf0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data_Team9.csv', delimiter=',', skiprows=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db1fe25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a323d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84940d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot data for visualization\n",
    "plt.figure()\n",
    "plt.scatter(data[:,0], data[:,1], color=\"Orange\", s=10, label= \"Data\")\n",
    "plt.xlabel(\"Time(Day)\")\n",
    "plt.ylabel(\"SN\")\n",
    "plt.title(\"Solar Cycle\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4139a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_points = data[:, 0]\n",
    "n_data = time_points.shape[0]\n",
    "data_points = data[:, 1]\n",
    "\n",
    "print(time_points)\n",
    "print(n_data)\n",
    "print(data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497fa00",
   "metadata": {},
   "source": [
    "## Initial conditions\n",
    "\n",
    "### parameters              \n",
    " Phase 1: T01, Ts1, Td1     \n",
    " Phase 2: T02, Ts2, Td2         \n",
    "...     \n",
    " Phase 10: T010, Ts10, Td10   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc70949",
   "metadata": {},
   "source": [
    "### Ts and Td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0 = np.array([Ts1, Td1, Ts2, Td2, ...])\n",
    "# here we use initial conditions given in the final project intro \n",
    "\n",
    "x0 = np.array([0.3, 5, 0.3, 5, 0.3, 5, 0.3, 5, 0.3, 5, 0.3, 5, 0.3, 5, 0.3, 5, 0.3, 5, 0.3, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc35e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The different parameters can be extracted as follows:\n",
    "Ts = x0[::2]\n",
    "Td = x0[1::2]\n",
    "\n",
    "print(Ts)\n",
    "print(Td)\n",
    "\n",
    "# Number of phases\n",
    "num_phases = len(Ts)\n",
    "num_phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391189b",
   "metadata": {},
   "source": [
    "### T0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1215c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make T0 array with initial conditions reported in Hathaway 2015\n",
    "T0array = (1878.916666666666667, 1890.166666666666667, 1902.000000000000000, 1913.500000000000000, 1923.583333333333333, 1933.666666666666667, 1944.083333333333333, 1954.250000000000000, 1964.750000000000000, 1976.166666666666667, 1986.666666666666667)\n",
    "# Create list of time intervals for each phase\n",
    "intervals = [(float(T0array[ix]),float(T0array[ix+1])) for ix in range(num_phases)]\n",
    "\n",
    "print(intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b448eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a loop to process all phases\n",
    "for ix, (a, b) in enumerate(intervals):\n",
    "    print(\"Processing phase\", ix+1, \"with interval (\", a, \",\", b, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f6eac",
   "metadata": {},
   "source": [
    "## Model and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9706af",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c68abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for multiple phases:\n",
    "def model(t, x):\n",
    "    Ts = x[::2]\n",
    "    Td = x[1::2]\n",
    "    num_phases = len(T0array)\n",
    "    intervals = [(T0array[ix],T0array[ix+1]) for ix in range(num_phases-1)]\n",
    "    #Ensure t is treated as an array for consistency\n",
    "    t = np.atleast_1d(t)\n",
    "    model_output = np.zeros_like(t)\n",
    "    for ix, (a,b) in enumerate(intervals):\n",
    "        # Create mask for current phase\n",
    "        mask = (a <= t) & (t < b)\n",
    "        # Apply model for current phase\n",
    "        model_output[mask] = ((t[mask] - T0array[ix])/Ts[ix])**2 * np.exp(-((t[mask] - T0array[ix])/Td[ix])**2)\n",
    "    if model_output.size == 1:\n",
    "        return model_output.item()\n",
    "    else:\n",
    "        return model_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69539606",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020667fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def mse(x):\n",
    "    return np.mean(np.square(data_points - model(time_points, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dbb4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "plt.figure()\n",
    "plt.scatter(time_points, data_points, color='orange', s=10)\n",
    "plt.plot(time_points, model(time_points, x0), color='blue', linestyle='--', linewidth=1)\n",
    "plt.show(block=False)\n",
    "\n",
    "# Initial mse:\n",
    "mse(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe4945b",
   "metadata": {},
   "source": [
    "## Hyper-parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd686c53",
   "metadata": {},
   "source": [
    "#### Quick exploration (showing our process)\n",
    "Before running a full sweep, we tested a few sigma and T0 values to find a reasonable range (avoiding unstable jumps and ensuring the loss decreases).\n",
    "We log the final MSE for each trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85499c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_log = []  # list of dicts\n",
    "\n",
    "def log_trial(T0, sigma, final_x):\n",
    "    trial_log.append({\n",
    "        \"T0\": float(T0),\n",
    "        \"sigma\": float(sigma),\n",
    "        \"final_mse\": float(mse(final_x))\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e928fcf2",
   "metadata": {},
   "source": [
    "### Test to find the sigma range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26543d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "T0 = 1\n",
    "sigma = 1\n",
    "outSA = sa_tune(x0, T0, sigma, mse, 1e4, thinning = 1)\n",
    "\n",
    "plt.figure()\n",
    "mse_curve = np.apply_along_axis(mse, 1, outSA)\n",
    "plt.plot(mse_curve)\n",
    "plt.show(block = False)\n",
    "\n",
    "mse(outSA[-1])\n",
    "log_trial(T0, sigma, outSA[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3626036",
   "metadata": {},
   "source": [
    "sigma works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d218cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "T0 = 0.1\n",
    "sigma = 0.00001\n",
    "outSA = sa_tune(x0, T0, sigma, mse, 1e4, thinning = 1)\n",
    "\n",
    "plt.figure()\n",
    "mse_curve = np.apply_along_axis(mse, axis=1, arr=outSA)\n",
    "plt.plot(mse_curve)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show(block=False)\n",
    "\n",
    "log_trial(T0, sigma, outSA[-1])\n",
    "mse(outSA[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trials = pd.DataFrame(trial_log).sort_values(\"final_mse\").reset_index(drop=True)\n",
    "df_trials.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e398da81",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning on the cluster (8×8 = 64)\n",
    "Load the 64 JSON outputs from the Slurm job array and pick the best (T0, sigma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_dir = Path(\"results_tuning\")\n",
    "tuning_files = sorted(tuning_dir.glob(\"tuning_*.json\"))\n",
    "\n",
    "assert len(tuning_files) == 64, f\"Expected 64 tuning files, found {len(tuning_files)}\"\n",
    "\n",
    "rows = []\n",
    "for path in tuning_files:\n",
    "    with path.open(\"r\") as f:\n",
    "        rows.append(json.load(f))\n",
    "\n",
    "df_tune = pd.DataFrame(rows).sort_values(\"final_mse\").reset_index(drop=True)\n",
    "df_tune[[\"idx\", \"T0\", \"sigma\", \"final_mse\", \"wall_time_sec\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc87b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = df_tune.iloc[0]\n",
    "T0_opt = best[\"T0\"]\n",
    "sigma_opt = best[\"sigma\"]\n",
    "x_opt_from_sweep = np.array(best[\"final_x\"], dtype=float)\n",
    "\n",
    "print(\"Best hyperparameters from 64-run sweep:\")\n",
    "print(\"T0_opt =\", T0_opt)\n",
    "print(\"sigma_opt =\", sigma_opt)\n",
    "print(\"best MSE =\", best[\"final_mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b32561e",
   "metadata": {},
   "source": [
    "#### Visual check of the best tuned solution\n",
    "We plot the best tuned model output against the data as a sanity check before final optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd39e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(time_points, data_points, color='orange', s=20)\n",
    "plt.plot(time_points, model(time_points, x_opt_from_sweep), color='blue')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8456c21",
   "metadata": {},
   "source": [
    "## Calibration on the cluster (parallel chains + speedup)\n",
    "Load calibration JSON files and extract the fitted parameters (center_of_mass). Plot wall time vs cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60b1fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_dir = Path(\"results_calibration\")\n",
    "calib_files = sorted(calib_dir.glob(\"calib_workers*_chains*.json\"))\n",
    "\n",
    "assert len(calib_files) > 0, \"No calibration result files found in results_calibration/\"\n",
    "\n",
    "rows = []\n",
    "for path in calib_files:\n",
    "    with path.open(\"r\") as f:\n",
    "        rows.append(json.load(f))\n",
    "\n",
    "df_calib = pd.DataFrame(rows).sort_values(\"n_workers\").reset_index(drop=True)\n",
    "df_calib[[\"n_workers\", \"wall_time_sec\", \"final_mse\", \"n_chains\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a570dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df_calib[\"n_workers\"], df_calib[\"wall_time_sec\"], marker=\"o\")\n",
    "plt.xlabel(\"Cores (cpus-per-task)\")\n",
    "plt.ylabel(\"Wall time (s)\")\n",
    "plt.show()\n",
    "\n",
    "t1 = float(df_calib[df_calib[\"n_workers\"]==1][\"wall_time_sec\"].iloc[0])\n",
    "df_calib[\"speedup\"] = t1 / df_calib[\"wall_time_sec\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(df_calib[\"n_workers\"], df_calib[\"speedup\"], marker=\"o\")\n",
    "plt.xlabel(\"Cores (cpus-per-task)\")\n",
    "plt.ylabel(\"Speedup (T1/Tp)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3118c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_final = df_calib.sort_values(\"n_workers\").iloc[-1]  # or choose lowest MSE\n",
    "center_of_mass = np.array(row_final[\"center_of_mass\"], dtype=float)\n",
    "\n",
    "print(\"Using calibration from\", row_final[\"n_workers\"], \"cores\")\n",
    "print(\"Calibration MSE:\", row_final[\"final_mse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a4d5e2",
   "metadata": {},
   "source": [
    "### correlation between Ts and Td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be9105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your model uses pairs: [Ts1, Td1, Ts2, Td2, ...]\n",
    "ts = center_of_mass[0::2]\n",
    "td = center_of_mass[1::2]\n",
    "\n",
    "slope, intercept = np.polyfit(ts, td, 1)\n",
    "td_fit = slope * ts + intercept\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(ts, td, s=20)\n",
    "plt.plot(ts, td_fit, label=f'Fit: td = {slope:.4f} * ts + {intercept:.4f}')\n",
    "plt.xlabel('Ts')\n",
    "plt.ylabel('Td')\n",
    "plt.legend()\n",
    "plt.show(block=False)\n",
    "\n",
    "slope, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6fec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(time_points, data_points, s=10, label=\"Data\")\n",
    "plt.plot(time_points, model(time_points, center_of_mass), label=\"Final calibrated model\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dbfae4",
   "metadata": {},
   "source": [
    "### R^2 coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89bea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = td - td_fit\n",
    "\n",
    "ss_total = np.sum((td - np.mean(td)) ** 2)\n",
    "ss_residual = np.sum(residuals ** 2)\n",
    "\n",
    "r_squared = 1 - (ss_residual / ss_total)\n",
    "print(f\"R² = {r_squared:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8891b6",
   "metadata": {},
   "source": [
    "### comparison to literature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6372fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the literature reports a slope per year vs per day (or vice versa),\n",
    "# you may need a rough rescaling depending on your time resolution.\n",
    "# Teacher example divides by 365 for ~daily resolution.\n",
    "\n",
    "slope_per_year_est = slope / 365.0\n",
    "print(\"slope =\", slope)\n",
    "print(\"slope/365 =\", slope_per_year_est)\n",
    "\n",
    "# Add your literature target value here once you have it:\n",
    "# print(\"Literature slope ~\", 0.02)  # example placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(time_points, center_of_mass)\n",
    "res = data_points - pred\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(time_points, res, s=10)\n",
    "plt.axhline(0)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Residual (data - model)\")\n",
    "plt.show(block=False)\n",
    "\n",
    "print(\"Final MSE =\", np.mean(res**2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
